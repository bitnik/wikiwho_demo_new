{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.lngselection import abbreviation\n",
    "from wikiwho_wrapper import WikiWho\n",
    "from external.wikipedia import WikipediaDV, WikipediaAPI\n",
    "from metrics.conflict import ConflictManager\n",
    "import numpy as np\n",
    "import random\n",
    "from BTM.script.topicDisplay import display_topics\n",
    "import tqdm\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# ## Some Extensions ##\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %store -r the_page\n",
    "\n",
    "global lng, the_page\n",
    "\n",
    "if 'the_page' not in locals():\n",
    "    import pickle\n",
    "    print(\"Loading default data...\")\n",
    "    the_page = pickle.load(open(\"data/the_page.p\",'rb'))\n",
    "\n",
    "lng = abbreviation('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiwho = WikiWho(lng=lng)\n",
    "all_content = wikiwho.dv.all_content(the_page['page_id'])\n",
    "revisions = wikiwho.dv.rev_ids_of_article(the_page['page_id'])\n",
    "\n",
    "con_manager = ConflictManager(all_content.copy(), \n",
    "                                           revisions.copy(), \n",
    "                                           lng=lng, \n",
    "                                           include_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_manager.calculate()\n",
    "token = con_manager.all_actions.copy()\n",
    "tokens_processed = token[['rev_id', 'rev_time', 'editor', 'token_id', 'token']].groupby(\"rev_id\")['token_id'].apply(lambda group_series: group_series.to_numpy()).reset_index()\n",
    "tokens_processed['token_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in tokens_processed[np.array(list(map(len,tokens_processed.token_id.values)))==1].iterrows():\n",
    "    k = random.choice([-1, 1])\n",
    "    np.append(tokens_processed.loc[i+k, 'token_id'], row['token_id'][0])\n",
    "tokens_processed = tokens_processed[np.array(list(map(len,tokens_processed.token_id.values)))>1]\n",
    "token_ids = token[['token', 'token_id']].drop_duplicates()['token_id'].to_numpy()\n",
    "X = tokens_processed['token_id'].to_numpy()\n",
    "vocab = token[['token', 'token_id']].drop_duplicates()['token'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing input ids for the C++ model\n",
    "\n",
    "X_max = np.max([np.max(x) for x in X])\n",
    "wf = open('BTM/input/input.txt', 'w')\n",
    "for x in tokens_processed['token_id']:\n",
    "    print(' '.join(map(str, [str(it) for it in x])), file=wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the vocab file for the C++ model\n",
    "vocab_dict = dict(zip(token_ids, vocab))\n",
    "with open('BTM/input/vocab.txt', 'w', newline='\\n') as f:\n",
    "    for i in range(X_max):\n",
    "        if i not in token_ids:\n",
    "            f.write(str(i) + \"\\t\" + \"oo\" + \"\\n\")\n",
    "        else:\n",
    "            f.write(str(i) + \"\\t\" + vocab_dict[i] + \"\\n\")\n",
    "\n",
    "# for l in open('../BTM/vocab.txt'):\n",
    "#     print(l.strip().split('\\t')[:2])\n",
    "#     voca[int(wid)] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT VARIABLES FOR THE MODEL #\n",
    "global niter, save_step, model_dir, doc_pt, voca_pt\n",
    "\n",
    "K=15   # number of topics\n",
    "alpha=0.1   \n",
    "beta=0.01\n",
    "niter=500    # number of iterations\n",
    "save_step=100    # number of steps after which to save\n",
    "\n",
    "model_dir='../output/model/'\n",
    "\n",
    "doc_pt='../input/input.txt' # path to the doc with token ids\n",
    "voca_pt='../input/vocab.txt' #path to the vocabulary\n",
    "\n",
    "W = X_max #vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the model\n",
    "%cd BTM/script\n",
    "\n",
    "!mkdir -p {model_dir}\n",
    "!make -C ../src/\n",
    "!../src/btm est {K} {W} {alpha} {beta} {niter} {save_step} {doc_pt} {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print output\n",
    "topics, _ = display_topics(model_dir, K, voca_pt, tokens_processed, lng, the_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global bow_corpus, dct\n",
    "\n",
    "\n",
    "def bow_corpus(token_list):\n",
    "    return [(_id, np.count_nonzero(token_list == _id)) for _id in token_list]\n",
    "\n",
    "bow_corpus = tokens_processed['token_id'].apply(bow_corpus)\n",
    "dct = Dictionary.from_corpus(bow_corpus)\n",
    "\n",
    "cm = CoherenceModel(topics=topics, corpus=bow_corpus, dictionary=dct, coherence='u_mass')\n",
    "coherence = cm.get_coherence()\n",
    "coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameter tuning\n",
    "\n",
    "# # Topics range\n",
    "# min_topics = 5\n",
    "# max_topics = 50\n",
    "# step_size = 5\n",
    "# topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# # Alpha parameter\n",
    "# alpha = list(np.arange(0.1, 5, 0.3))\n",
    "\n",
    "# # Beta parameter\n",
    "# beta = list(np.arange(0.01, 1, 0.3))\n",
    "\n",
    "# model_results = { 'Topics': [],\n",
    "#                  'Alpha': [],\n",
    "#                  'Beta': [],\n",
    "#                  'Coherence_C_V': [],\n",
    "#                  'Coherence_C_U_mass': []\n",
    "#                 }\n",
    "\n",
    "# #texts in format list of lists of str (with token ids)\n",
    "# texts = tokens_processed['token_id'].apply(lambda x: x.tolist()).tolist()\n",
    "# texts = [[str(item) for item in text] for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C_v coherence measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_coherence_values(k, W, a, b, tokens_processed):\n",
    "#     global niter, save_step, model_dir, doc_pt, voca_pt, bow_corpus, dct, lng, the_page, texts\n",
    "#     !mkdir -p {model_dir}\n",
    "#     !make -C ../src/\n",
    "#     !../src/btm est {k} {W} {a} {b} {niter} {save_step} {doc_pt} {model_dir}\n",
    "#     topics, _ = display_topics(model_dir, k, voca_pt, tokens_processed, lng, the_page)\n",
    "#     cm_cv = CoherenceModel(topics=topics, texts = texts, corpus=bow_corpus, dictionary=dct, coherence='c_v')\n",
    "#     cm_umass = CoherenceModel(topics=topics, corpus=bow_corpus, dictionary=dct, coherence='u_mass')\n",
    "#     c_v = cm_cv.get_coherence()\n",
    "#     c_u_mass = cm_umass.get_coherence()\n",
    "#     return c_v, c_u_mass\n",
    "\n",
    "# if 1 == 1:\n",
    "#     pbar = tqdm.tqdm(total=540)\n",
    "\n",
    "#     # iterate through number of topics\n",
    "#     for k in topics_range:\n",
    "#         # iterate through alpha values\n",
    "#         for a in alpha:\n",
    "#             # iterare through beta values\n",
    "#             for b in beta:\n",
    "#                 # get the coherence score for the given parameters\n",
    "#                 c_v, c_u_mass = compute_coherence_values(k, W, a, b, tokens_processed)\n",
    "#                 # Save the model results\n",
    "#                 model_results['Topics'].append(k)\n",
    "#                 model_results['Alpha'].append(a)\n",
    "#                 model_results['Beta'].append(b)\n",
    "#                 model_results['Coherence_C_V'].append(c_v)\n",
    "#                 model_results['Coherence_C_U_mass'].append(c_u_mass)\n",
    "\n",
    "#                 pbar.update(1)\n",
    "#     btm_cv = pd.DataFrame(model_results)#.to_csv('btm_tuning_results.csv', index=False)\n",
    "#     pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#btm_cv.to_csv('btm_tuning_results.csv', index=False)\n",
    "btm_cv = pd.read_csv('btm_tuning_results.csv')\n",
    "btm_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btm_cv.iloc[btm_cv['Coherence_C_U_mass'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btm_cv.iloc[btm_cv['Coherence_C_V'].idxmax()]  #high coherence because tokens mostly appear together in most of the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Graph of coherence U_Mass:')\n",
    "fig, axs = plt.subplots(3,1, figsize=(10, 10))\n",
    "\n",
    "btm_topics = btm_cv.groupby('Topics')['Coherence_C_U_mass'].agg('max')\n",
    "btm_alpha = btm_cv.groupby('Alpha')['Coherence_C_U_mass'].agg('max')\n",
    "btm_beta = btm_cv.groupby('Beta')['Coherence_C_U_mass'].agg('max')\n",
    "btm_topics.plot.line(x='Topics', y='Coherence_C_U_mass', ax=axs[0])\n",
    "btm_alpha.plot.line(x='Alpha', y='Coherence_C_U_mass', ax=axs[1])\n",
    "btm_beta.plot.line(x='Bets', y='Coherence_C_U_mass', ax=axs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Graph of coherence C_V:')\n",
    "fig, axs = plt.subplots(3,1, figsize=(10, 10))\n",
    "\n",
    "btm_topics = btm_cv.groupby('Topics')['Coherence_C_V'].agg('max')\n",
    "btm_alpha = btm_cv.groupby('Alpha')['Coherence_C_V'].agg('max')\n",
    "btm_beta = btm_cv.groupby('Beta')['Coherence_C_V'].agg('max')\n",
    "btm_topics.plot.line(x='Topics', y='Coherence_C_V', ax=axs[0])\n",
    "btm_alpha.plot.line(x='Alpha', y='Coherence_C_V', ax=axs[1])\n",
    "btm_beta.plot.line(x='Bets', y='Coherence_C_V', ax=axs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model \n",
    "!mkdir -p {model_dir}\n",
    "!make -C ../src/\n",
    "!../src/btm est {10} {W} {2.8} {0.31} {niter} {save_step} {doc_pt} {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output of model with best parameters\n",
    "best_topics, orig_topics = display_topics(model_dir, 10, voca_pt, tokens_processed, lng, the_page)\n",
    "\n",
    "best_topics_tokens = [[vocab_dict[int(token)] for token in tokens] for tokens in best_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "i = 0\n",
    "best_topics_format = []\n",
    "for it in orig_topics.values():\n",
    "    items_split = it[0].split()\n",
    "    items_format = [(item.split(':')[0], float(item.split(':')[1])) for item in items_split]\n",
    "    best_topics_format.append((i, items_format))\n",
    "    i += 1\n",
    "\n",
    "#topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(best_topics_format[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim, spacy, logging, warnings\n",
    "# import gensim.corpora as corpora\n",
    "\n",
    "# id2word = corpora.Dictionary(best_topics_tokens)\n",
    "\n",
    "# # Create Corpus: Term Document Frequency\n",
    "# corpus = [id2word.doc2bow(text) for text in best_topics_tokens]\n",
    "\n",
    "# # Build LDA model\n",
    "# lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "#                                            id2word=id2word,\n",
    "#                                            num_topics=4, \n",
    "#                                            random_state=100,\n",
    "#                                            update_every=1,\n",
    "#                                            chunksize=10,\n",
    "#                                            passes=10,\n",
    "#                                            alpha='symmetric',\n",
    "#                                            iterations=100,\n",
    "#                                            per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# from bokeh.plotting import figure, output_file, show\n",
    "# from bokeh.models import Label\n",
    "# from bokeh.io import output_notebook\n",
    "\n",
    "# # Get topic weights\n",
    "# topic_weights = []\n",
    "# for i, row_list in enumerate(lda_model[corpus]):\n",
    "#     topic_weights.append([w for i, w in row_list[0]])\n",
    "\n",
    "# # Array of topic weights    \n",
    "# arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "# # Keep the well separated points (optional)\n",
    "# arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# # Dominant topic number in each doc\n",
    "# topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# # tSNE Dimension Reduction\n",
    "# tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "# tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "# # Plot the Topic Clusters using Bokeh\n",
    "# output_notebook()\n",
    "# n_topics = 4\n",
    "# mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "# plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "#               plot_width=900, plot_height=700)\n",
    "# plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "# show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_weights = []\n",
    "# for i, row_list in enumerate(lda_model[corpus]):\n",
    "#     print(i, row_list)\n",
    "#     topic_weights.append([w for i, w in row_list[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
